仔细阅读从exp/gspo.sh启动脚本开始的代码逻辑，明白gspo的实现方式。以及整体代码的训练逻辑。我一会需要你帮我修改算法。从这个脚本开始，然后阅读实际执行的代码和文件，搞清楚细节


可是他不是需要完整的logits参与计算吗，会不会导致显存爆炸啊？你看下原本的代码流程我记得他算重要性的时候，应该得到了old_θ和θ，里面算过完整的logits的可是如果对原有的logits进行操作，不是会创建一个新的对象，大小是batch，seq_len，词汇表，依旧会爆显存啊，除非是原地操作，你确定可以完整计算，并且没有额外显存开销



我们通过引入一种自适应机制来彻底解决此问题，该机制能基于每个序列内在的风险，为其动态地调整γ值。我们使用**变异系数（Coefficient of Variation, CV）**来量化此风险，这是一个衡量相对波动的无量纲指标：
CV
i
=
σ
w
μ
w
=
std
(
{
w
i
,
t
}
)
mean
(
{
w
i
,
t
}
)
CV 
i
​
 = 
μ 
w
​
 
σ 
w
​
 
​
 = 
mean({w 
i,t
​
 })
std({w 
i,t
​
 })
​
 
随后，每个序列i的自适应风险规避系数γ_i被定义为：
γ
i
=
clamp
(
C
⋅
CV
i
,
 
0
,
 
γ
max
)
γ 
i
​
 =clamp(C⋅CV 
i
​
 , 0, γ 
max
​
 )
此处，C是一个全局的风险敏感度缩放因子（默认值为1.0），而γ_max是为了保证数值稳定性而设定的上限。此设计使我们的算法——我们称之为自适应风险敏感性重要性校正 (Adaptive RSIC, A-RSIC)——仅在检测到高方差权重时才表现出强烈的风险规避，而在处理稳定样本时其行为则接近于标准的均值。这是自适应？然后RSIC要怎么算？我之前给你的详细指南里是不是都提到过了要如何实现？请把我思考，用内存友好，计算量友好的方式实现，并且不必过于复杂，实现创新点思路的情况下，能真的提高训练稳定性和质量最重要的，现在你自己综合考虑，并给我你要如何实现的详细思路，在什么地方替换什么，怎么替换，怎么计算，怎么节省内存和计算量，是不是最合理，给我分析，我同意后你再写代码。

























好的，你要记住下面几件事情，第一在保证创新点完成的情况下，能确实提升训练稳定性和精度是最重要的，其次要注意内存使用不要引入过多额外开销，还有就是lse的成熟实现，要注意lse的数值稳定性。最后就是请将原来的算法注释掉，然后新的算法接着写，并提示我如何才能手动的方便的注释和反注释，这样我就可以自由的切换我的创新点和gspo去做对比了。